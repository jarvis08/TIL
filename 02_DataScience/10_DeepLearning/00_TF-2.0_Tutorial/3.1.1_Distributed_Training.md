# 3.1.1 Distributed Training

https://www.tensorflow.org/beta/guide/keras/overview?hl=ko

[`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras?hl=ko) 모델은 [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy?hl=ko)를 사용하여 다중 GPU에서 실행할 수 있습니다. 이 API는 기존 코드를 거의 수정하지 않고 다중 GPU에서 훈련을 분산시킬 수 있습니다.

현재는 [`tf.distribute.MirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy?hl=ko)가 유일하게 지원되는 분산 전략입니다. `MirroredStrategy`는 한 대의 장치에서 계산 결과를 모두 수집하는 방식인 그래프 내 복제(in-graph replication)를 수행합니다. [`distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy?hl=ko)를 사용하려면 `Strategy`의 `.scope()` 안에 옵티마이저 객체 생성, 모델 구성, 컴파일 단계를 포함시킨 다음 모델을 훈련합니다.

다음 코드는 한 대의 컴퓨터에서 다중 GPU를 사용해 [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model?hl=ko)을 분산 처리하는 예입니다.

먼저, `MirroredStrategy`의 `scope()` 안에서 모델을 정의합니다:

```python
strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
  model = tf.keras.Sequential()
  model.add(layers.Dense(16, activation='relu', input_shape=(10,)))
  model.add(layers.Dense(1, activation='sigmoid'))

  optimizer = tf.keras.optimizers.SGD(0.2)

  model.compile(loss='binary_crossentropy', optimizer=optimizer)

model.summary()
```

```
Model: "sequential_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_21 (Dense)             (None, 16)                176       
_________________________________________________________________
dense_22 (Dense)             (None, 1)                 17        
=================================================================
Total params: 193
Trainable params: 193
Non-trainable params: 0
_________________________________________________________________
```

그다음, 보통 때와 같은 데이터로 모델을 훈련합니다:

```python
x = np.random.random((1024, 10))
y = np.random.randint(2, size=(1024, 1))
x = tf.cast(x, tf.float32)
dataset = tf.data.Dataset.from_tensor_slices((x, y))
dataset = dataset.shuffle(buffer_size=1024).batch(32)

model.fit(dataset, epochs=1)
```

```
Train on 32 steps
32/32 [==============================] - 2s 53ms/step - loss: 0.6984

<tensorflow.python.keras.callbacks.History at 0x7fc9c067def0>
```

더 자세한 정보는 [텐서플로의 분산 훈련 가이드](https://www.tensorflow.org/beta/guide/distribute_strategy?hl=ko)를 참고하세요.

