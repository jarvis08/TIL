# 2.3.5 Callbacks

## 1. 콜백(Callback)이란

### 1-1. 콜백의 정의

콜백 함수의 정의는 다음과 같습니다.

```
어떤 함수에 전달된 후, 그 함수가 실행되는 동안 특정 조건을 충족하면 호출되는 함수
```

텐서플로(TensorFlow)에서의 콜백 또한 크게 다르지 않으며, 다양한 방법으로 학습하는 모델을 모니터링하고 조작할 수 있습니다. 텐서플로에는 콜백 함수를 두 가지 방법으로 사용할 수 있습니다.

- 제공되는 내장 콜백 사용
- `tf.keras.callbacks.Callback`를 상속하여 사용자 정의(Custom) 콜백 제작

현재 챕터에서는 내장되어 있는 콜백들에 대해 알아보겠습니다.



### 1-2. 내장 콜백의 종류

콜백들은 종류 별로 클래스 형태로 제작되어 있으며, 매개변수를 지정할 수 있으며, 메서드를 실행할 수 있습니다.

| 콜백 종류               | 내용                                                      |
| ----------------------- | --------------------------------------------------------- |
| `BaseLogger`            | 에포크의 평균을 누적하며 콜백                             |
| `CSVLogger`             | 에포크의 결과를 csv 파일로 전송                           |
| `Callback`              | 새로운 콜백을 제작할 수 있는 클래스                       |
| `EarlyStopping`         | 모니터링하던 대상의 성능 향상이 멈추면 학습을 중단        |
| `History`               | `History` 객체에 발생하는 특징적인 상황을 기록            |
| `LambdaCallback`        | 간단한 사용자 정의 콜백을 즉시 생성                       |
| `LearningRateScheduler` | 학습율(Learning Rate)을 동적으로 조정                     |
| `ModelCheckpoint`       | 매 에포크마다 모델을 저장                                 |
| `ProgbarLogger`         | 성능 평가 지표(metrics)를 stdout으로 출력                 |
| `ReduceLROnplateau`     | 성능 평가 지표의 성능 향상이 멈췄을 때 학습율 감소를 적용 |
| `RemoteMonitor`         | 서버에 이벤트를 전송                                      |
| `TensorBoard`           | 텐서보드(TensorBoard)로 시각화 하는 기능을 사용           |
| `TerminateOnNaN`        | `NaN` 손실이 발생했을 때 학습을 종료                      |





##  2. BaseLogger

`tf.layers.callbacks.BaseLogger(stateful_metrics=None).method`

### 2-1. 매개변수

`stateful_metrics` : 에포크 마다 평가 척도인 metric들을 평균내게 되는데, 여기에 지정해둔 척도들은 평균되어지지 않으며 있는 그대로 로그에 기록됩니다.



### 2-2. 메서드

`on_batch_begin`, `on_batch_end`, `on_batch_end`, `on_epoch_begin`, `on_epoch_end`, `on_predict_batch_begin`, `on_predict_batch_end`, `on_predict_begin` ...

다음 두 가지 매개변수를 가지는 메서드들이다. `batch='현재 에포크에서 기록할 배치의 인덱스'`와`log='로그에 기록 할 척도(metric)'`



### 2-2. 종류 별 코드 예시

`Tensorboard`의 경우 '3.4.1 Tensorboard' 에서 자세히 다루겠습니다.







---

## **BaseLogger and History**

You get these without doing anything special — they’re applied to the model by default. By convention, most of the time you’ll just call `model.fit()`on a model object without assigning the result to anything. Instead of calling `model.fit()`, you can write `my_history = model.fit()`. The `my_history` variable is assigned a `keras.callbacks.History` object. The `history`property of this object is a dict with average accuracy and average loss information for each epoch. You can also inspect the `params`property, which is a dict of the parameters used to fit the model.

## **ModelCheckpoint**

This callback will save your model as a checkpoint file (in `hdf5` format) to disk after each successful epoch. You can actually set the output file to be dynamically named based on the epoch. You can also write either the loss value or accuracy value as part of the log’s file name. This is especially handy for models where epochs take an extremely long time, or if you’re running them on AWS spot instances and the spot instance price rises above your maximum bid.

![img](https://miro.medium.com/max/60/1*6OVpCFwlXmBQYio4yiLQ6g.png?q=20)

![img](https://miro.medium.com/max/2508/1*6OVpCFwlXmBQYio4yiLQ6g.png)

Plot of accuracy (top) and loss (bottom) over 100 epochs training an RNN

## CSVLogger

The CSVLogger is pretty self-explanatory. It writes a CSV log file containing information about epochs, accuracy, and loss to disk so you can inspect it later. It’s great if you want to roll your own graphs or keep a record of your model training process over time. The graph above was created from one of these files.

## **EarlyStopping**

One technique to reduce overfitting in neural networks is to use early stopping. Early stopping prevents overtraining of your model by terminating the training process if it’s not really learning anything. This is pretty flexible — you can control what metric to monitor, how much it needs to change to be considered “still learning”, and how many epochs in a row it can falter before the model stops training.

## **RemoteMonitor**

This callback sends JSON status messages via an HTTP POST method. This could be easily integrated with a pub/sub messaging service like Kafka or a queue like Amazon SQS. For instance, you could setup a lightweight HTTP endpoint and route any POSTed JSON as a payload to an SQS queue. You could then have another process monitor the queue to kick off other processes or to handle specific events.

## **LearningRateScheduler**

Another optimization technique with deep learning is to adjust the learning rate over time. The learning rate determines the size of the steps taken during the gradient descent process.

One method is to start with a relatively large value, and decrease it in later training epochs. All you need to do is write a simple function that returns the desired learning rate based on the current epoch, and pass it as the `schedule` parameter to this callback.

![img](https://miro.medium.com/max/60/1*fDthtjnQCjHfe14dvfkkUg.png?q=20)

![img](https://miro.medium.com/max/1120/1*fDthtjnQCjHfe14dvfkkUg.png)

TensorBoard visualization of accuracy by training epoch for three separate training runs of a recurrent neural network

## TensorBoard

This is probably the coolest of all the out-of-the-box callbacks. By using a TensorBoard callback, logs will be written to a directory that you can then examine with TensorFlow’s excellent TensorBoard visualization tool. It even works (to an extent) if you’re using a backend other than TensorFlow, like Theano, or CNTK (if you’re a glutton for punishment).

## **LambdaCallback**

If none of the techniques above fit what you’re trying to achieve, you can create your own callback. If you just want to create a basic one, you can use LambdaCallback. This allows you to trigger events when an epoch, batch, or training process begins or ends.

![img](https://miro.medium.com/max/56/1*4xXZONeCviw2gotsnOj-Tg.png?q=20)

![img](https://miro.medium.com/max/700/1*4xXZONeCviw2gotsnOj-Tg.png)

If you want to create something a little more complex, you can create your own callback by inheriting from the `keras.callbacks.Callback` class. This is only for those who truly want low-level control over how a callback is executed. You may also want to create your own callback if you’re implementing some custom logic that’s not available in the existing callbacks.