# 2.3.4 Compile and Training

이번 챕터에서는 '학습에 사용할 **데이터**'와 '학습되는 **모델**'이 준비된 이후, **어떻게** 모델을 학습시킬 것인가를 결정하는 과정에 대해 알아보겠습니다. 학습은 `Model` 클래스(Class)의 메서드를 사용하여 진행하며, 학습 과정에는 크게 컴파일(Compile)과 학습(Train)이 있습니다. 컴파일 메서드에는 `compile`이 있으며, 학습 메서드로는 `fit`, `fit_generator`, 그리고 `train_on_batch`가 있습니다.

먼저 각각의 메서드들에 대해 알아본 후, 메서드들을 어떻게 모델 학습에 사용하는지 알아보겠습니다.

<br><br>

## 1. 컴파일

### 1.1 컴파일의 기능

학습을 하기 위한 모델의 환경을 설정합니다.

`model.compile()`은 사용자가 제작한 모델을 어떻게 학습시킬 것인지를 결정하는 모델 클래스의 메서드입니다. 학습 방법은 아래의 인자(Parameter)들을 조절하여 결정할 수 있습니다.

```python
compile(
    optimizer,
    loss=None,
    metrics=None,
    loss_weights=None,
    sample_weight_mode=None,
    weighted_metrics=None,
    target_tensors=None,
    distribute=None,
    **kwargs
)
```

그 중에서도 가장 중요한 세 가지 매개변수는 `optimizer`, `loss`, `metrics`입니다.

`optimizer` 는 주로 학습하는 손실 함수의 계산 결과를 바탕으로 학습하는 변수에 어떻게 변화를 줄 것인가를 결정하며, 그 예로 Adam, SGD 등이 있습니다.

`loss`는 최적화 과정에서 최소화 시키는 손실 함수(loss function)를 설정합니다. 손실 함수는 모델의 예측 결과와 실제 결과값 사이의 차이를 수치로 나타내며, 평균 제곱 오차(mse), cross entropy 등이 그 예시입니다.

`metrics`는 학습 과정을 모니터링 하기 위해 사용되며, accuracy와 같은 척도를 사용합니다.

<br>

### 1.2 compile 매개변수

```python
compile(
    optimizer,
    loss=None,
    metrics=None,
    loss_weights=None,
    sample_weight_mode=None,
    weighted_metrics=None,
    target_tensors=None,
    distribute=None,
    **kwargs
)
```

- **`optimizer`**

  `tf.keras.optimizers` 중 필요한 최적화 객체를 사용할 수 있으며, 직접 구현한 옵티마이저의 이름을 `String` 형태로 호출할 수 있습니다.

- **`loss`**

  `tf.losses.Loss` 중 필요한 손실 함수를 사용할 수 있으며, 직접 구현한 손실 함수의 이름을 `String` 형태로 호출할 수 있습니다.

- **`metrics`**

  학습과 테스트 과정에서 평가되는 측정 지표의 리스트를 지정합니다. 만약 여러 결과값을 산출하는 모델을 사용한다면, 딕셔너리 형태로 결과값들을 지정하여 다른 측정 지표를 사용하게 할 수 있습니다. 다음은 딕셔너리 형태의 지정 예시입니다.

  `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`

- `loss_weights`

  모델을 학습하며 계산되는 손실에 가중치를 적용합니다.

- `sample_weight_mode`

  기본값은 `sample_wise weights(1D)`이며, `timestep-wise weighting(2D)` 사용을 원할시 매개변수 값을 `"temporal"`로 지정합니다.

- `weighted_metrics`

  학습 및 테스트 과정에서 설정하는 `sample_weight`와 `class_weight`로 인해 가중 될 측정 수단의 리스트를 지정합니다.

- `target_tensors`

  학습 과정에서 타겟 데이터와 함께 사용되는 모델의 타겟이며, 기본값 사용시 케라스가 자동적으로 생성합니다. 사용자가 텐서를 지정할 수 있으며, 텐서의 리스트, 텐서를 지정하는 {이름:텐서} 쌍의 딕셔너리 형태 또한 가능합니다.

- `distribute`

  분산학습을 위한 매개변수이며, **텐서플로 2.0 버전에서는 지원하지 않습니다**. 따라서 현재 다루고 있는 컴파일 메서드에 값을 부여하는 것이 아니라, 모델 자체를 분산 학습 환경으로 구성해야 합니다.

- `**kwargs`

  추가적인 인자를 딕셔너리 형태로 부여할 수 있습니다.

<br>

### 1.3 사용 예시

다음 코드 블록은 `model.compile()`의 사용 예시입니다.

```python
model.compile(optimizer=tf.keras.optimizers.Adam(0.01),
              loss='mse',
              metrics=['mae'])
```

위 코드는 모델을 Adam Optimizer를 사용하며 학습 변수의 변화를 평균 제곱 오차(mse)로 계산하고, 학습 성과를 평균 절댓값 오차로 나타나게 합니다.

```python
model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),
              loss=tf.keras.losses.CategoricalCrossentropy(),
              metrics=[tf.keras.metrics.CategoricalAccuracy()])
```

위 코드는 모델을 RMSprop(Root mean square prop)을 사용하여 최적화 하고, 크로스 엔트로피 손실 함수를 이용하여 성능의 오차를 측정합니다. 그리고 분류 모델의 정확도를 계산하여 나타냅니다.

<br>

<br>

## 2. 핏(fit)을 이용한 학습

### 2.1 핏의 기능

먼저 지시했던 데이터셋, 최적화 방식, 손실 함수 등을 사용하여 학습을 진행합니다.

```python
fit(
    x=None,
    y=None,
    batch_size=None,
    epochs=1,
    verbose=1,
    callbacks=None,
    validation_split=0.0,
    validation_data=None,
    shuffle=True,
    class_weight=None,
    sample_weight=None,
    initial_epoch=0,
    steps_per_epoch=None,
    validation_steps=None,
    validation_freq=1,
    max_queue_size=10,
    workers=1,
    use_multiprocessing=False,
    **kwargs
)
```

`fit` 메서드에도 세 개의 중요한 매개변수가 있습니다. 여기서 설명하지 않은 매개변수들은 3. 핏 제너레이터(fit_generator)에서 자세히 알아보겠습니다.

- `epochs`: 훈련은 epoch로 구성됩니다. 한 epoch는 전체 입력 데이터를 한번 순회하는 것입니다(작은 배치로 나누어 수행됩니다).
- `batch_size`: 넘파이 데이터를 전달하면 모델은 데이터를 작은 배치로 나누고 훈련 과정에서 이 배치를 순회합니다. 이 정수 값은 배치의 크기를 지정합니다. 전체 샘플 개수가 배치 크기로 나누어 떨어지지 않으면 마지막 배치의 크기는 더 작을 수 있습니다.
- `validation_data`: 모델의 프로토타입(prototype)을 만들 때는 검증 데이터(validation data)에서 간편하게 성능을 모니터링해야 합니다. 입력과 레이블(label)의 튜플을 이 매개변수로 전달하면 에포크가 끝날 때마다 추론 모드(inference mode)에서 전달된 데이터의 손실과 측정 지표를 출력합니다.

<br>

### 2.2 fit 사용 예시

아래의 코드블록은 train 데이터를 `data`, `data` 별 label을 `labels`, epoch를 `10`으로, 배치 크기를 `32`로 설정한 예시입니다.

```python
model.fit(data, labels, epochs=10, batch_size=32)
```

다음의 코드 블록은 validation 데이터를 부여하는 예시 코드입니다.

```python
model.fit(data, labels, epochs=10, batch_size=32,
          validation_data=(val_data, val_labels))
```

<br>

<br>

## 3. 핏 제너레이터(fit_generator)를 이용한 학습

### 3.1 핏 제너레이터의 기능

제너레이터(Generator)에 의해 배치들로 산출된 데이터를 모델에 핏 합니다. 즉, `fit_generator`는 데이터를 불러올 때 파이썬 제너레이터 혹은 `keras.utils.Sequence` 객체를 사용하여 데이터를 모델에 공급합니다.

```python
fit_generator(
    generator,
    steps_per_epoch=None,
    epochs=1,
    verbose=1,
    callbacks=None,
    validation_data=None,
    validation_steps=None,
    validation_freq=1,
    class_weight=None,
    max_queue_size=10,
    workers=1,
    use_multiprocessing=False,
    shuffle=True,
    initial_epoch=0
)
```

`fit`과 `fit_generator`의 가장 큰 차이는 데이터 전달 방식에 있습니다. `fit`의 경우 미리 저장해 두었던 데이터를 `data`와 `label`을 사용하여 전달받지만, `fit_generator`를 통해 데이터를 확보합니다. `fit_generator`에 대해 자세히 알아보기 전에 `generator`가 무엇인지 간단히 알아보겠습니다.

<br>

### 3.2 제너레이터를 사용하는 이유

다음은 Python Documentation의 [Python Generator](https://docs.python.org/ko/3.6/reference/datamodel.html?highlight=generator%20yield) 정의입니다.

>  제너레이터 함수, Generator Functions
>
> [`yield`](https://docs.python.org/ko/3.6/reference/simple_stmts.html#yield) 문([yield 문](https://docs.python.org/ko/3.6/reference/simple_stmts.html#yield) 절 참조)을 사용하는 함수나 메서드를 *제너레이터 함수 (generator function)* 라고 부른다. 이런 함수를 호출하면 항상 이터레이터(iterator) 객체를 돌려주는데, 함수의 바디(body)를 실행하는 데 사용된다: 이터레이터의 [`iterator.__next__()`](https://docs.python.org/ko/3.6/library/stdtypes.html#iterator.__next__) 메서드를 호출하면 [`yield`](https://docs.python.org/ko/3.6/reference/simple_stmts.html#yield) 문이 값을 제공할 때까지 함수가 실행된다. 함수가 [`return`](https://docs.python.org/ko/3.6/reference/simple_stmts.html#return) 문을 실행하거나 끝에 도달하면 [`StopIteration`](https://docs.python.org/ko/3.6/library/exceptions.html#StopIteration) 예외를 일으키고, 이터레이터는 반환하는 값들의 끝에 도달하게 된다.

간단히 말하자면, 제너레이터는 iterator를 생성하여 반환합니다. 하지만 이러한 설명 만으로는 제너레이터가 어떤 역할을 수행할 수 있는지 이해가 잘 되지 않습니다. 그리고 다음과 같은 의문이 들 수 있습니다.

'`fit`을 사용할 때와 같이, 데이터를 서로 다른 변수에 분할해서 저장해둔 후 필요할 때마다 호출하여 사용하는게 편하지 않나요? 왜 그런 복잡해 보이는 함수를 굳이 사용해야 하는거죠? '

이에 대한 가장 명쾌한 답변은, '하드웨어 자원은 무한히 제공되지 않는다' 입니다. 우리가 사용하는 메모리는 용량이 제한되어 있기 때문에 데이터가 대용량일 경우 위의 간단한 변수 할당 방법은 제한될 수 밖에 없습니다.

우리는 제너레이터를 사용하여 메모리 자원의 한계를 회피할 수 있습니다. 모델은 전체 데이터를 작은 배치들로 나누어 학습을 진행하는데, 이 때 현재의 iter에서는 하나의 배치 데이터만을 사용하게 됩니다. 그리고 제너레이터는 **지금 당장 필요한 만큼의 데이터만** 메모리에 유지시킬 수 있습니다. 따라서 우리의 모델은 전체 데이터가 아닌, 배치 데이터 만큼의 메모리만 사용하여 대용량 데이터를 학습할 수 있습니다.

현재 챕터의 목적은 `fit_generator`를 사용하여 학습하는 방법을 알아보는 것이므로, 제너레이터 자체의 자세한 사용 방법은 넘어가도록 하겠습니다.

<br>

### 3.3 fit_generator 사용하기

제너레이터는 모델 학습을 진행함과 동시에(parallel) 동작하며 데이터를 제공합니다. 예를 들자면, 모델이 GPU를 사용하여 학습을 진행하는 중에도 제너레이터는 CPU를 사용하여 Data Augmentation을 진행할 수 있습니다.

`fit_generator`의 매개변수들을 다시 한번 살펴보겠습니다.

```python
fit_generator(
    generator,
    steps_per_epoch=None,
    epochs=1,
    verbose=1,
    callbacks=None,
    validation_data=None,
    validation_steps=None,
    validation_freq=1,
    class_weight=None,
    max_queue_size=10,
    workers=1,
    use_multiprocessing=False,
    shuffle=True,
    initial_epoch=0
)
```

- `generator`

  제너레이터 혹은 `keras.utils.Sequence`의 `Sequence` 인스턴스를 사용하여 멀티프로세싱을 사용할 때 데이터의 중복을 피할 수 있ㅇㅇ습니다. `fit_generator`가 받을 `generator`의 결과물(output)은 다음 두 가지 형태(튜플) 중 하나이어야 합니다.

  - `(inputs, targets)`
  - `(inputs, targets, sample_weights)`

  위 형태와 같은 `generator`의 결과물은 `fit_generator`가 하나의 배치로 처리합니다. 하나의  epoch는 `steps_per_epoch`에 의해 결정됩니다.

- `steps_per_epoch`

  하나의 epoch가 끝나고 새로운 epoch가 시작하기 전, `generator`의 산출 회수이자 배치의 개수입니다. 이 값은 전체 데이터셋을 배치 사이즈로 나눈 값과 동일해야 합니다. 위에서 언급한 `Sequence` 객체를 `generator`로 사용할 경우, 기본값(default)으로 len(generator)를 사용하게 됩니다.

- `epochs`

  데이터셋 전체를 총 몇 회 반복하여 학습할 것인지를 설정합니다.

- `verbose`

  `0`, `1`, `2` 세 개의 값을 부여할 수 있으며, 학습 중 출력되는 문구의 형태를 설정합니다.

- `callbacks`

  학습 과정에서 어떤 callback이 사용될 지 리스트(list) 형태로 설정합니다.

- `validation_data`

  validation data를 다음 세 가지 방법으로 부여합니다.

  - 제너레이터
  - 튜플 `(inputs, targets)`
  - 튜플 `(inputs, targets, sample_weights)`

- `validation_steps`

  `validation_data`가 제너레이터 형태일 경우에만 사용하는 매개변수입니다. 제너레이터가 멈추기 전 총 몇 회 반복할 지를 결정합니다. `Sequence` 객체를 사용할 경우 기본값으로 `len(validation_data)`를 사용합니다.

- `validation_freq`

  `validation_data`가 존재할 경우에만 사용하는 매개변수입니다. 다음 두 가지 형태로 사용할 수 있습니다.

  - 정수 형태로 부여하여 다음 validation이 실행되기 전까지 몇 회의 epoch를 학습할 것인지 결정
  - `collections.Container` 객체를 사용하여 특정 회차의 epoch에 validation을 수행할 것인지 결정

- `class_weight`

  학습 중 클래스(class) 별 loss 계산에 가중치(실수)를 부여할 수 있습니다.

- `max_queue_size`

  제너레이터의 큐(queue)의 크기를 결정하며, 기본값은 `10`입니다.

- `workers`

  process-based threading을 사용할 때 최대 프로세스 사용 개수를 설정합니다. 기본값은 `1`이며, `0`으로 설정할 경우 제너레이터를 메인 쓰레드에서 실행합니다.

- `use_multiprocessing`

  기본값은 `False`이며, `True`일 경우 process-based threading을 사용합니다.

- `shuffle`

  `True`일 경우 epoch 별 시작 배치의 순서를 변경합니다. `generator` 매개변수가 `Sequence`의 객체일 경우에만 사용 가능하며, `steps_per_epoch`가 `none`으로 설정되어야 합니다.

- `initial_epoch`

  시작 epoch를 설정할 수 있으며, 이는 학습 중단 후 재개하는 상황에서 유용하게 사용할 수 있는 매개변수입니다.

<br>

### 3-4. 예시

```python
def generate_batch(path):
    while 1:
        f = open(path)
        for line in f:
            x1, x2, y = process_line(line)
            yield ({'input_1': x1, 'input_2': x2}, {'output': y})
        f.close()

model.fit_generator(generate_batch('/my_file.txt'),
                                steps_per_epoch=10000, epochs=10)
```

`generate_batch` 함수는 `fit_generator`가 실행되는 동안 함께 실행 상태를 유지합니다. 그리고 Iteration이 진행되어 `fit_generator`가 `generate_batch` 함수를 호출할 때마다, 정해진 양(배치 사이즈)의 배열을 생성(`process_line`)하고, `yield`를 통해 반환합니다. 위의 코드를 봤을 때 `generate_batch` 함수는 `open`한 파일을 1회 순회하는 시점에 종료되며, 그 이전에는 함수를 종료하지 않습니다. 따라서 중복된 인덱스의 데이터를 반환하는 일이 발생하지 않습니다.

<br><br>

## 4. 배치 학습

`train_on_batch`는 하나의 배치 데이터로 학습을 한 회 진행합니다. 보다 정확히는, 한 배치 데이터를 모델에 적용하여 도출해 낸 결과를 이용하여 가중치를 1회 업데이트합니다.

```python
train_on_batch(
    x,
    y=None,
    sample_weight=None,
    class_weight=None,
    reset_metrics=True
)
```

매개변수들에 대해 알아보겠습니다.

- `x`

  배치 데이터를 넣는 공간이며, 다음의 형태들을 사용할 수 있습니다.

  - 넘파이(Numpy) 배열 혹은 넘파이 배열들을 담은 리스트
  - 텐서 혹은 텐서들의 리스트
  - 모델이 입력 데이터들에 이름을 부여했을 경우, 이름과 실제 데이터로 이루어진 딕셔너리
  - `tf.data` 데이터셋 혹은 데이터셋의 iterator

- `y`

  입력 데이터에 해당하는 목표 및 정답 데이터를 부여하는 매개변수입니다. 이 매개변수에 부여되는 데이터의 형태는 입력 데이터와 동일해야 합니다. 즉, 입력 데이터가 넘파이 배열일 경우 정답 데이터 또한 넘파이 배열이어야 합니다.

- `sample_weight`

  `x` 매개변수에 부여한 입력 데이터 별로 어떤 가중치의 학습에 사용할 지 지정합니다.

- `class_weight`

  가중치에 가중치를 부여하여 특정 클래스를 더 학습거나 덜 학습학습하도록 지시합니다.

- `reset_metrics`

  `Ture`일 경우 측정 수단(metric)은 이 배치의 학습 결과만을 반환합니다.

  `False`일 경우 다른 배치들의 학습과 함께 누적됩니다.

<br><br>

## 5. 예시 코드

데이터셋을 준비하고, 모델을 구성한 후 컴파일 및 학습시키는 과정은 텐서플로(TensorFlow) 공식 홈페이지의 [Image Classification](https://www.tensorflow.org/tutorials/keras/basic_classification) 예시 코드를 기반으로 설명하겠습니다. 학습 과정은 총 4 단계입니다.

1. **데이터셋 준비**

   데이터 정제, Train/Validation/Test 용도 별 분리 등의 전처리 작업을 진행합니다.

   여기서 사용 할 예시 데이터셋은 패션 MNIST 데이터셋입니다.

   ```python
   from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals
   
   # tensorflow와 tf.keras를 임포트
   import tensorflow as tf
   from tensorflow import keras
   
   # 헬퍼(helper) 라이브러리를 임포트
   import numpy as np
   import matplotlib.pyplot as plt
   
   # 텐서플로에서 제공하는 패션 MNIST 데이터셋 다운로드 및 용도 별 분리
   fashion_mnist = keras.datasets.fashion_mnist
   (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
   ```

   ```python
   # 정규화(Normalization)를 위해 이미지 데이터의 값을 0~1 사이 값으로 조정
   train_images = train_images / 255.0
   
   test_images = test_images / 255.0
   ```

2. **모델 제작**

   학습 시킬 모델을 조직합니다. 층(layer)는 직접 제작하거나, API를 통해 이미 제작되어져 있는 것들을 활용할 수 있습니다. 예시에서는 API를 사용합니다.

   ```python
   model = keras.Sequential([
       keras.layers.Flatten(input_shape=(28, 28)),
       keras.layers.Dense(128, activation=tf.nn.relu),
       keras.layers.Dense(10, activation=tf.nn.softmax)
   ])
   ```

3. **컴파일**

   `compile` 메서드를 이용하여 학습 방법(옵티마이저, Optimizer)과 학습 결과 측정 방법(손실 함수), 모니터 도구(학습 성과 지표)를 설정합니다.

   ```python
   model.compile(optimizer='adam',
                 loss='sparse_categorical_crossentropy',
                 metrics=['accuracy'])
   ```

4. **학습**

   `fit`, `fit_generator` 등의 메서드를 통해 컴파일 된 모델을 train 데이터를 사용하여 학습합니다.

   ```python
   model.fit(train_images, train_labels, epochs=5)
   ```

5. 정확도 평가 / 테스트

   ```python
   test_loss, test_acc = model.evaluate(test_images, test_labels)
   print('테스트 정확도:', test_acc)
   ```

   결과

   ```python
   10000/10000 [==============================] - 0s 38us/sample - loss: 0.3369 - accuracy: 0.8758
   테스트 정확도: 0.8758
   ```

   