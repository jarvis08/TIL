# Variable

## 1. 변수, Variable이란

### 1-1. TensorFlow 공식 홈페이지 설명

> A TensorFlow **variable** is the best way to represent shared, persistent state manipulated by your program.
>
> Variables are manipulated via the `tf.Variable` class. A `tf.Variable` represents a tensor whose value can be changed by running ops on it. Specific ops allow you to read and modify the values of this tensor. Higher level libraries like `tf.keras` use `tf.Variable` to store model parameters.

> TensorFlow **변수**는 프로그램에 의해 변화하는 공유된 지속 상태를 표현하는 가장 좋은 방법이다.
>
> 변수는 [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) 클래스에서 처리된다. 하나의 [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable)은 하나의 텐서를 표현하는데, 텐서값은 텐서에 연산을 수행하여 변경시킬 수 있다. 특정한 연산은 이 텐서값을 읽고 수정한다. [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) 같은 좀 더 고수준의 라이브러리는 모델 파라미터를 저장하는데 [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable)을 사용한다.



### 1-2. tf.Varible을 사용하는 이유

`tf.Variable`은 그 이름으로 부터 알 수 있듯이, 변수를 저장할 수 있는 객체입니다. 그런데 왜 일반적인 파이썬 코딩처럼 `w = 0`과 같은 형태로 변수를 선언하면 안되는 것일까요? 그 이유는 텐서플로(TensorFlow)가 모델을 학습할 때 텐서(Tensor)와 그래프(Graph)를 활용하기 때문입니다. 그리고 우리는 우리가 선언 할 변수들을 텐서 형태로 다루기 위해 `Variable` 객체를 사용해야 합니다. `tf.Variable`을 사용하여 선언한 변수는 사라지지 않고 지속되며, 공유됩니다. 사용자가 작성한 연산의 묶음(프로그램)은 작성된 연산을 수행하여 `tf.Variable`의 값을 갱신해 나아갑니다.



## 2. Variable 다루기 

### 2-1. 변수 의 생성 및 할당

`tf.Variable` 객체를 생성하기 위해서는 초기값을 선언해 주어야 하며, 그 형태는 다음과 같습니다.

`변수명 = tf.Variable(초기값)` 

아래의 코드 블럭은 텐서플로 공식 홈페이지에 작성된 코드 예시입니다.

```python
import tensorflow as tf

my_variable = tf.Variable(tf.zeros([1., 2., 3.]))
```

위 코드는 `my_variable`이라는 변수에 `tf.Variable` 객체를 저장합니다. `tf.Variable` 객체는 shape이 [1, 2, 3]이며, 모든 값이 0으로 이루어진 데이터의 텐서입니다. 데이터 타입(`dtype`, data type)은 따로 지정되지 않았으므로 `1.` 이라는 실수의 값을 참고하여 자동으로 `tf.float32`라는 실수를 의미하는 객체로 저장됩니다.

- 특정 장치를 지정하여 생성 및 저장하기

  `tf.device(할당 위치)`를 통해 원하는 장치에 변수를 저장할 수 있습니다. 지정하지 않는다면 자동으로 가장 빠른 장치에 저장되며, GPU가 가용하다면 대부분의 변수들이 자동으로 GPU에 저장됩니다.

  ```python
  import tensorflow as tf
  
  with tf.device('/device:GPU:1'):
      v = tf.Variable(tf.zeros([10, 10]))
  ```

  참고. `tf.distribute` API를 사용하여 다양한 분산 설정을 하는 것이 이상적인 장치 할당 방법입니다.

  

### 2-2. 변수 조작하기

1. `tf.Tensor`처럼 사용하는 방법

   ```python
   import tensorflow as tf
   
   v = tf.Variable(0.0)
   w = v + 1
   tf.print(w)
   ```

   결과

   ```
   1
   ```

2. 메서드를 활용하는 방법

   ```python
   import tensorflow as tf
   
   v = tf.Variable(0.0)
   v.assign_add(2)
   
   tf.print(v)
   
   t = v.read_value()
   print(t)
   ```

   결과

   ```
   <tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=2.0>
   2
   tf.Tensor(2.0, shape=(), dtype=float32)
   ```

   - 대부분의 텐서플로 옵티마이저(optimizer)는 특정 알고리즘에 대해 변수값을 효율적으로 업데이트 할 수 있는 연산을 보유하고, 사용하고 있습니다.

   

### 2-3. 사용 중인 변수 목록 확인하기

`tf.Module` 클래스(Class)를 상속받으면 `variables` 혹은 `trainable_variables` 메서드를 사용하여 사용중인 모든 변수들을 return 받음으로서 추적(track)할 수 있습니다.

```python
import tensorflow as tf

class MyModule(tf.Module):
  def __init__(self):
    self.v0 = tf.Variable(1.0)
    self.vs = [tf.Variable(x) for x in range(10)]
    
class AnotherModule(tf.Module):
  def __init__(self):
    self.m = MyModule()
    self.v = tf.Variable(10.0)
    
my_var = AnotherModule()
print(len(my_var.variables))
```

결과

```
12
```

`MyModule` 클래스에는 11개(1 + 10)의 데이터가 존재하며, `AnotherModule` 클래스에는 `MyModule` 클래스로 부터 받은 11개의 데이터와 자기 자신의 1개의 데이터를 합한 총 12개의 데이터가 존재합니다.

층(layer)을 형성하고 있다면, `tf.Module` 대신 `tf.keras.Layer`를 상속받는 것이 더 좋은 선택일 수 있습니다. 케라스 인터페이스를 형성함으로서 케라스에 완전하게 통합될 수 있으며, 이로 인해 `model.fit`을 포함하여 잘 통합된 다른 API들을 사용할 수 있습니다. `tf.keras.Layer`의 변수 추적 방법은 `tf.Module`의 방법과 동일합니다.



## 3. tf.Variable 초기화(Initialization)

### 3-1. 초기화 메서드

```python
  def __init__(self,
               initial_value=None,
               trainable=None,
               validate_shape=True,
               caching_device=None,
               name=None,
               variable_def=None,
               dtype=None,
               import_scope=None,
               constraint=None,
               synchronization=VariableSynchronization.AUTO,
               aggregation=VariableAggregation.NONE,
               shape=None):
```

`__init__` 메서드는 tf.Variable를 생성할때 필수적으로 실행되는 메서드입니다. 이 메서드에 대해 이해가 되지 않으신다면, 파이썬의 클래스 초기화에 대해 추가적으로 공부해 보시기 바랍니다.

여러 초기값(default)이 지정되어 있는데, 우리는 이 인자들 중 필요한 값들을 골라 적절히 변경해 주어야 합니다. 생성한 변수는 graph collections의 `collections`라는 저장 장소에 추가됩니다. `collections`에 저장된 변수들은 `[GraphKeys.GLOBAL_VARIABLES]`의 기본값(defaults)으로 제공됩니다. 만약 `trainable`이 `True`라면, 변수는 graph collection의 `GraphKeys.TRAINABLE_VARIABLES`에도 등록됩니다.



### 3-2. 인자(Parameter) 설명

- `initial_value`

  변수의 초기값을 지정하는 인자이며, 텐서 혹은 텐서로 변환 가능한 파이썬 객체를 인수로 사용할 수 있습니다.

  `validate_shape=False`라고 선언하지 않는 이상, 인수로 사용한 초기값의 shape은 `shape` 인자에 할당한 값과 일치해야 합니다. `validate_shape` 인자와 `shape` 인자는 아래에서 설명하겠습니다.

  인수가 없이도 호출하는 것이 가능합니다. 단, `dtype`은 반드시 지정해 주어야 합니다.

- `trainable`

  GradientTapes는 자동적으로 변수들의 `trainable` 속성을 확인합니다.

  기본값은 `True `이며, `synchronization`이 `ON_READ`로 설정되었을 경우에만 기본값이 `False`로 설정됩니다.

- `validate_shape`

  만약 이 인자의 인수로 False를 부여한다면, shape을 지정하지 않은 채 변수를 초기화 할 수 있습니다.

  기본값인 `True`로 설정 할 경우 `initial_value`의 shape을 반드시 지정해야 합니다.

- `caching_device`

  변수가 캐시(cache)되는 장소를 특정 짓고 싶다면, 그 장치를 string 형태로 지정 할 수 있습니다. 기본값은 변수의 장치입니다.

- `name`

  변수에게 이름(별칭)을 부여 할 수 있습니다. 기본값은 `Variable`이며, 모든 변수가 기본값 이름을 따를 지라도 자동으로 구분됩니다.

- `variable_def`

  `VariableDef` 프로토콜(Protocol) 버퍼(Buffer)입니다. 특정 변수 객체를 인수로 지정한다면, 이는 지정된 변수 객체의 내용을 이용하여 새로운 변수로 재생성함을 의미합니다. 인수로 지정하는 변수 객체는 그래프 속에 이미 존재하는 변수 노드들 중 하나여야 합니다.

  해당 인자의 활용 여부는 그래프에 아무런 영향을 미치지 않으며, 다른 인자들과도 상호배타적입니다.

- `dtype`

  만약 `dtype`이 설정된다면 `initial_value`는 설정한 데이터 타입으로 변환됩니다.

  기본값인 `None`을 사용함과 동시에 `initial_value`로 텐서가 지정된다면, 데이터 타입은 그 텐서의 타입으로 지정됩니다. 텐서가 아니라면, 변수 객체를 텐서로 변환하는 `convert_to_tensor` 과정에서 자동으로 결정됩니다.

- `import_scope`

  프로토콜 버퍼에서 초기화하는 경우에만 사용되며, 이름 공간(Name Scope)의 `Variable`에 추가하도록 지시하는 인자입니다. string 형태로 지정할 수 있습니다.

- `constraint`

  옵티마이저(`Optimizer`)에 의해 값이 갱신된 이후 변수 객체에 적용할 수 있는 선택적인 투사(projection) 함수의 실행 여부입니다. 선택적인 함수의 예로 층의 가중치(weight)에 norm constraints를 적용하거나 value constraints를 적용하는 함수가 있습니다.

  이 함수들은 변수의 값을 표상하며, 아직 투사되지 않은(unprojected) 텐서들을 입력값으로 취합니다. 반환값은 그 텐서들이 투사된 값입니다.

  비동기 분산 학습을 진행할 때 이 함수들을 사용하는 것은 적절하지 않을 수 있습니다.

- `synchronization`

  이 인자를 통해 분산된 변수들을 언제 집계할 지 지시할 수 있습니다.

  인수로는 `tf.VariableSynchronization` 클래스에서 정의된 상수만 사용 가능합니다.

  기본값은 `AUTO`로 설정되어 있는데, 이는 현재의 `DistributionStrategy`가 언제 동기화를 진행할 지 결정하게 합니다.

- `aggregation`

  분산된 변수가 어떻게 집계될 지 지시합니다.

  인수로는 `tf.VariableAggregation` 클래스에서 정의된 상수만 사용 가능합니다.

- `shape`

  변수의 shape을 지정합니다.

  만약 `None`으로 설정된다면, shape은 `initial_value`의 shape으로 설정됩니다.

  `tf.TensorfShape(None)`으로 설정하여 특정되지 않은 shape을 부여한다면, 변수 객체는 다른 shape들로 할당될 수 있습니다.

  

### 3-3. 발생할 수 있는 에러의 종류

- `ValueError`

  만약  `variable_def`와 `initial_value` 모두가 지정되었을 경우 발생

- `ValueError`

  `validate_shape=True`인 상황에서, `initial value`가 지정되지 않았거나 shape이 없을 경우 발생

- `RuntimeError`

  eager execution이 활성화 되었을 경우 발생





## 4. 텐서플로 API의 변수 생성 방법

실제로 사용되고 있는 텐서플로의 딥러닝 네트워크 또한 우리가 살펴본 `tf.Variable`과 같이 변수를 생성하는지 알아보겠습니다. 코드 내부를 자세히 살펴보는 것은 많은 사전 지식을 요구하므로, '정말 사용되고 있는가'에 대해 확인해 보는 차원에서 마무리 하겠습니다.



### 4-1. 가중치

먼저 우리가 알아 볼 변수의 종류인 '가중치'에 대해 간단히 설명드리겠습니다. 딥러닝 네트워크의 모델은 여러 개의 층으로 이루어져 있으며, 각각의 층은 여러 개의 셀(cell)로 구성됩니다. 그리고 각각의 셀에는 여러 개의 가중치들이 있습니다. 수 많은 가중치들은 딥러닝 네트워크가 학습하는 대상이며, 학습되어 변화해야하기 때문에 변수로써 생성됩니다.



### 4-2. 가중치 생성 코드

텐서플로의 깃헙(Github)에서 가중치를 생성하는 코드를 찾아보겠습니다. 텐서플로에서 제공하는 많은 셀들 중에서 SimpleRNNCell 클래스를 예시로 살펴보겠습니다. 아래의 코드 블록은 SimpleRNNCell 클래스에서 사용하는 셀 생성 메서드인 `build`의 코드입니다.

```python
  def build(self, input_shape):
    self.kernel = self.add_weight(
        shape=(input_shape[-1], self.units),
        name='kernel',
        initializer=self.kernel_initializer,
        regularizer=self.kernel_regularizer,
        constraint=self.kernel_constraint)
    self.recurrent_kernel = self.add_weight(
        shape=(self.units, self.units),
        name='recurrent_kernel',
        initializer=self.recurrent_initializer,
        regularizer=self.recurrent_regularizer,
        constraint=self.recurrent_constraint)
    if self.use_bias:
      self.bias = self.add_weight(
          shape=(self.units,),
          name='bias',
          initializer=self.bias_initializer,
          regularizer=self.bias_regularizer,
          constraint=self.bias_constraint)
    else:
      self.bias = None
    self.built = True
```

메서드 이름으로 부터 알 수 있듯이, 우리가 주목해야 할 부분은 `add_weight()`입니다. 다시 `add_weight()` 함수가 정의된 곳을 찾아가 보면, `Layer`라는 클래스안에 `add_weight()` 함수가 정의되어 있는 것을 알 수 있습니다. `Layer` 클래스에 대해 설명하는 Docstring을 살펴보면 다음과 같은 문구가 있습니다.

```python
""" This is the class from which all layers inherit."""
```

`Layer` 클래스는 모든 모델들이 층을 구성하려면 상속 받아야 하는 기본적이며 핵심적인 틀입니다. 그리고 그 안에 정의된 메서드인 `add_weight()`은 `variable`을 생성하여 반환합니다. 다음 코드 블록은 `add_weight()`의 선언 부분입니다.

```python
  def add_weight(self,
                 name=None,
                 shape=None,
                 dtype=None,
                 initializer=None,
                 regularizer=None,
                 trainable=None,
                 constraint=None,
                 partitioner=None,
                 use_resource=None,
                 synchronization=tf_variables.VariableSynchronization.AUTO,
                 aggregation=tf_variables.VariableAggregation.NONE,
                 **kwargs):
```

완벽히 동일한 매개 변수를 사용하지는 않지만, '가중치'라는 특수한 역할의 변수 또한 `tf.Variable`과 매우 유사한 작업을 통해 변수를 생성하고 있음을 확인할 수 있습니다.